{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:24:19.534100Z",
     "iopub.status.busy": "2025-05-16T09:24:19.533792Z",
     "iopub.status.idle": "2025-05-16T09:24:29.000853Z",
     "shell.execute_reply": "2025-05-16T09:24:28.999833Z",
     "shell.execute_reply.started": "2025-05-16T09:24:19.534076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepface in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0.93)\n",
      "Requirement already satisfied: openai in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.90.0)\n",
      "Requirement already satisfied: requests>=2.27.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (2.32.4)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (2.1.3)\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (2.2.3)\n",
      "Requirement already satisfied: gdown>=3.10.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (5.2.0)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (4.67.1)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (11.2.1)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (4.11.0.86)\n",
      "Requirement already satisfied: tensorflow>=1.9.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (2.19.0)\n",
      "Requirement already satisfied: keras>=2.2.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (3.10.0)\n",
      "Requirement already satisfied: Flask>=1.1.2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (3.1.1)\n",
      "Requirement already satisfied: flask-cors>=4.0.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (6.0.1)\n",
      "Requirement already satisfied: mtcnn>=0.1.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (1.0.0)\n",
      "Requirement already satisfied: retina-face>=0.0.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (0.0.17)\n",
      "Requirement already satisfied: fire>=0.4.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (0.7.0)\n",
      "Requirement already satisfied: gunicorn>=20.1.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deepface) (23.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fire>=0.4.0->deepface) (3.1.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=1.1.2->deepface) (8.2.1)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=1.1.2->deepface) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=1.1.2->deepface) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click>=8.1.3->Flask>=1.1.2->deepface) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown>=3.10.1->deepface) (4.13.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown>=3.10.1->deepface) (3.18.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=2.2.0->deepface) (2.3.0)\n",
      "Requirement already satisfied: rich in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=2.2.0->deepface) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=2.2.0->deepface) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=2.2.0->deepface) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=2.2.0->deepface) (0.16.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=2.2.0->deepface) (0.5.1)\n",
      "Requirement already satisfied: joblib>=1.4.2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mtcnn>=0.1.0->deepface) (1.5.1)\n",
      "Requirement already satisfied: lz4>=4.3.3 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mtcnn>=0.1.0->deepface) (4.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.23.4->deepface) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.23.4->deepface) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.27.1->deepface) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.27.1->deepface) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (5.29.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (80.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow>=1.9.0->deepface) (2.19.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->deepface) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->deepface) (0.7.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=2.2.0->deepface) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deepface openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:24:29.002411Z",
     "iopub.status.busy": "2025-05-16T09:24:29.002153Z",
     "iopub.status.idle": "2025-05-16T09:24:38.069037Z",
     "shell.execute_reply": "2025-05-16T09:24:38.068401Z",
     "shell.execute_reply.started": "2025-05-16T09:24:29.002383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have tensorflow 2.19.0 and this requires tf-keras package. Please run `pip install tf-keras` or downgrade your tensorflow.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\retinaface\\commons\\package_utils.py:19\u001b[39m, in \u001b[36mvalidate_for_keras3\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\n\u001b[32m     21\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtf_keras is already available - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf_keras.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deepface\\DeepFace.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommons\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m package_utils, folder_utils\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommons\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Logger\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     modeling,\n\u001b[32m     22\u001b[39m     representation,\n\u001b[32m     23\u001b[39m     verification,\n\u001b[32m     24\u001b[39m     recognition,\n\u001b[32m     25\u001b[39m     demography,\n\u001b[32m     26\u001b[39m     detection,\n\u001b[32m     27\u001b[39m     streaming,\n\u001b[32m     28\u001b[39m     preprocessing,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     32\u001b[39m logger = Logger()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deepface\\modules\\modeling.py:16\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# project dependencies\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfacial_recognition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     VGGFace,\n\u001b[32m      7\u001b[39m     OpenFace,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     GhostFaceNet,\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mface_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     FastMtCnn,\n\u001b[32m     18\u001b[39m     MediaPipe,\n\u001b[32m     19\u001b[39m     MtCnn,\n\u001b[32m     20\u001b[39m     OpenCv,\n\u001b[32m     21\u001b[39m     Dlib \u001b[38;5;28;01mas\u001b[39;00m DlibDetector,\n\u001b[32m     22\u001b[39m     RetinaFace,\n\u001b[32m     23\u001b[39m     Ssd,\n\u001b[32m     24\u001b[39m     Yolo,\n\u001b[32m     25\u001b[39m     YuNet,\n\u001b[32m     26\u001b[39m     CenterFace,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdemography\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Age, Gender, Race, Emotion\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspoofing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FasNet\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deepface\\models\\face_detection\\RetinaFace.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mretinaface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetinaFace \u001b[38;5;28;01mas\u001b[39;00m rf\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mDetector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Detector, FacialAreaRegion\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# pylint: disable=too-few-public-methods\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\retinaface\\RetinaFace.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mretinaface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommons\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m package_utils\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# users should install tf_keras package if they are using tf 2.16 or later versions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mpackage_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_for_keras3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m logger = Logger(module=\u001b[33m\"\u001b[39m\u001b[33mretinaface/RetinaFace.py\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# pylint: disable=global-variable-undefined, no-name-in-module, unused-import, too-many-locals, redefined-outer-name, too-many-statements, too-many-arguments\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# configurations\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SHASHANK\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\retinaface\\commons\\package_utils.py:24\u001b[39m, in \u001b[36mvalidate_for_keras3\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtf_keras is already available - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf_keras.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# you may consider to install that package here\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     25\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou have tensorflow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and this requires \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtf-keras package. Please run `pip install tf-keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor downgrade your tensorflow.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: You have tensorflow 2.19.0 and this requires tf-keras package. Please run `pip install tf-keras` or downgrade your tensorflow."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:24:38.070925Z",
     "iopub.status.busy": "2025-05-16T09:24:38.070428Z",
     "iopub.status.idle": "2025-05-16T09:24:38.076157Z",
     "shell.execute_reply": "2025-05-16T09:24:38.075325Z",
     "shell.execute_reply.started": "2025-05-16T09:24:38.070901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Class for Extracting Facial Features using Resnet\n",
    "class FacialFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_features=10):\n",
    "        super(FacialFeatureExtractor, self).__init__()\n",
    "        # Use a pretrained ResNet as base\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the final fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Add custom layers for specific facial features\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, num_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for feature probabilities\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:24:38.077815Z",
     "iopub.status.busy": "2025-05-16T09:24:38.077530Z",
     "iopub.status.idle": "2025-05-16T09:24:38.095350Z",
     "shell.execute_reply": "2025-05-16T09:24:38.094555Z",
     "shell.execute_reply.started": "2025-05-16T09:24:38.077785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Given The Image , DeepFace will Analyze for Facial Features\n",
    "def analyze_facial_features(image_path):\n",
    "\n",
    "    try:\n",
    "        analysis = DeepFace.analyze(img_path=image_path, \n",
    "                                   actions=['age', 'gender', 'race', 'emotion'],\n",
    "                                   enforce_detection=False,\n",
    "                                   detector_backend='retinaface')\n",
    "        \n",
    "        # Extract key information\n",
    "        basic_features = {\n",
    "            'age': analysis[0]['age'],\n",
    "            'gender': analysis[0]['gender'],\n",
    "            'dominant_race': analysis[0]['dominant_race'],\n",
    "            'dominant_emotion': analysis[0]['dominant_emotion'],\n",
    "            'emotion_scores': analysis[0]['emotion']\n",
    "        }\n",
    "    \n",
    "        return {**basic_features}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {image_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:24:38.096211Z",
     "iopub.status.busy": "2025-05-16T09:24:38.095992Z",
     "iopub.status.idle": "2025-05-16T09:24:38.114458Z",
     "shell.execute_reply": "2025-05-16T09:24:38.113886Z",
     "shell.execute_reply.started": "2025-05-16T09:24:38.096193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Using Actor's Name and API Prompt, Finding Actor Traits based on the Actor's Roles in Different Movies\n",
    "def get_actor_role_traits(actor_name):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze actor {actor_name}'s most notable roles and identify 20 common character traits \n",
    "    they often portray in movies. Focus on personality traits, physical characteristics they're \n",
    "    known for, and types of roles they excel in. Format the response as a JSON list of traits.\n",
    "    \"\"\"\n",
    "\n",
    "    token = '#YOUR API KEY'\n",
    "    endpoint = \"https://models.github.ai/inference\"\n",
    "    model_name = \"openai/gpt-4o\"\n",
    "    \n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            base_url=endpoint,\n",
    "            api_key=token,\n",
    "        )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            temperature=1.0,\n",
    "            top_p=1.0,\n",
    "            max_tokens=5000,\n",
    "            model=model_name\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting traits for {actor_name}: {str(e)}\")\n",
    "        return \"API_LIMIT_REACHED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:24:38.115418Z",
     "iopub.status.busy": "2025-05-16T09:24:38.115140Z",
     "iopub.status.idle": "2025-05-16T09:24:38.132190Z",
     "shell.execute_reply": "2025-05-16T09:24:38.131369Z",
     "shell.execute_reply.started": "2025-05-16T09:24:38.115399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# From Base Directory, Extract Actor's Name, Actor's Path\n",
    "def extract_actor_names(base_dir):\n",
    "    actor_names = []\n",
    "    actor_paths = {}\n",
    "    \n",
    "    for actor_folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, actor_folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            actor_names.append(actor_folder)\n",
    "            actor_paths[actor_folder] = folder_path\n",
    "    \n",
    "    return actor_names, actor_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:24:38.133038Z",
     "iopub.status.busy": "2025-05-16T09:24:38.132861Z",
     "iopub.status.idle": "2025-05-16T09:24:38.147442Z",
     "shell.execute_reply": "2025-05-16T09:24:38.146723Z",
     "shell.execute_reply.started": "2025-05-16T09:24:38.133022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For each Actor's Image and path, extract its individual Image files and then analyze facial features using DeepFace\n",
    "def process_actor_images(actor_name, actor_path):\n",
    "  \n",
    "    image_features = []\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(actor_path) \n",
    "                  if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(actor_path, img_file)\n",
    "        features = analyze_facial_features(img_path)\n",
    "        if features:\n",
    "            image_features.append(features)\n",
    "    \n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:24:38.149249Z",
     "iopub.status.busy": "2025-05-16T09:24:38.149019Z",
     "iopub.status.idle": "2025-05-16T09:24:38.170107Z",
     "shell.execute_reply": "2025-05-16T09:24:38.169319Z",
     "shell.execute_reply.started": "2025-05-16T09:24:38.149231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Given Facial Features from DeepFace analysis, Aggregate features will give a cumulitive feature from all the images gathered\n",
    "\n",
    "def aggregate_features(facial_features):\n",
    "  \n",
    "    if not facial_features:\n",
    "        return {}\n",
    "    \n",
    "    # Initialize aggregated features\n",
    "    aggregated = {}\n",
    "    \n",
    "    # Numeric features to average\n",
    "    numeric_features = ['age', 'eye_intensity', 'face_seriousness', 'strong_look']\n",
    "    \n",
    "    # Categorical features to take most common\n",
    "    categorical_features = ['gender', 'dominant_race', 'dominant_emotion']\n",
    "    \n",
    "    # Boolean features (handle separately)\n",
    "    boolean_features = ['has_beard']\n",
    "    \n",
    "    # Process numeric features\n",
    "    for feature in numeric_features:\n",
    "        if all(feature in item for item in facial_features):\n",
    "            aggregated[feature] = sum(item[feature] for item in facial_features) / len(facial_features)\n",
    "    \n",
    "    # Process categorical features\n",
    "    for feature in categorical_features:\n",
    "        if all(feature in item for item in facial_features):\n",
    "            # Count occurrences\n",
    "            counts = {}\n",
    "            for item in facial_features:\n",
    "                value = item[feature]\n",
    "                # Make sure value is hashable (not a dict)\n",
    "                if not isinstance(value, dict):\n",
    "                    counts[value] = counts.get(value, 0) + 1\n",
    "            \n",
    "            # Find most common if counts is not empty\n",
    "            if counts:\n",
    "                most_common = max(counts.items(), key=lambda x: x[1])[0]\n",
    "                aggregated[feature] = most_common\n",
    "    \n",
    "    # Process boolean features\n",
    "    for feature in boolean_features:\n",
    "        if all(feature in item for item in facial_features):\n",
    "            # Count True values\n",
    "            true_count = sum(1 for item in facial_features if item[feature])\n",
    "            # Set to True if majority are True\n",
    "            aggregated[feature] = true_count > len(facial_features) / 2\n",
    "    \n",
    "    # Special handling for emotion scores\n",
    "    if all('emotion_scores' in item for item in facial_features):\n",
    "        emotion_scores = {}\n",
    "        # Get all possible emotions from the first item\n",
    "        if facial_features and len(facial_features) > 0 and 'emotion_scores' in facial_features[0]:\n",
    "            emotions = facial_features[0]['emotion_scores'].keys()\n",
    "            \n",
    "            for emotion in emotions:\n",
    "                # Calculate average score for each emotion\n",
    "                scores = [item['emotion_scores'].get(emotion, 0) for item in facial_features]\n",
    "                emotion_scores[emotion] = sum(scores) / len(scores)\n",
    "            \n",
    "            aggregated['emotion_scores'] = emotion_scores\n",
    "    # Add gender feature explicitly \n",
    "    if all('gender' in item for item in facial_features):\n",
    "        gender_counts = {}\n",
    "        for item in facial_features:\n",
    "            gender = item['gender']\n",
    "            # Check if gender is a dictionary\n",
    "            if isinstance(gender, dict):\n",
    "                # Either extract a specific value from the dictionary\n",
    "                # or convert it to a string representation\n",
    "                gender = str(gender)  # or some other appropriate conversion\n",
    "            gender_counts[gender] = gender_counts.get(gender, 0) + 1\n",
    "    \n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:24:38.171368Z",
     "iopub.status.busy": "2025-05-16T09:24:38.171081Z",
     "iopub.status.idle": "2025-05-16T09:24:38.188041Z",
     "shell.execute_reply": "2025-05-16T09:24:38.187214Z",
     "shell.execute_reply.started": "2025-05-16T09:24:38.171342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save Current Progress\n",
    "def save_progress(processed_actors):\n",
    "\n",
    "    # Save processed actors list\n",
    "    with open(\"processed_actors.json\", \"w\") as f:\n",
    "        json.dump(processed_actors, f, indent=2)\n",
    "    \n",
    "    # Save checkpoint with timestamp\n",
    "    checkpoint = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"processed_count\": len(processed_actors)\n",
    "    }\n",
    "    \n",
    "    with open(\"actor_processing_checkpoint.json\", \"w\") as f:\n",
    "        json.dump(checkpoint, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:28:15.515735Z",
     "iopub.status.busy": "2025-05-16T09:28:15.515424Z",
     "iopub.status.idle": "2025-05-16T09:28:15.522294Z",
     "shell.execute_reply": "2025-05-16T09:28:15.521397Z",
     "shell.execute_reply.started": "2025-05-16T09:28:15.515709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Processing Actors with given base_dir\n",
    "def process_actors_batch(base_dir):\n",
    "    \n",
    "    actor_names, actor_paths = extract_actor_names(base_dir)\n",
    "    \n",
    "    processed_actors = {}\n",
    "    if os.path.exists(\"/kaggle/input/actorsfinal/cleaned_data.json\"):\n",
    "        with open(\"/kaggle/input/actorsfinal/cleaned_data.json\", \"r\") as f:\n",
    "            processed_actors = json.load(f)\n",
    "    \n",
    "    # Load the complete list of actors\n",
    "    all_actors = []\n",
    "    with open(\"/kaggle/input/indian-actor-images-dataset/List of Actors.txt\", \"r\") as f:\n",
    "        all_actors = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    print(processed_actors.keys())\n",
    "    processed_actor_names = set(processed_actors.keys())\n",
    "    remaining_actors = [actor for actor in all_actors if actor not in processed_actor_names]\n",
    "    \n",
    "    print(f\"Total actors in list: {len(all_actors)}\")\n",
    "    print(f\"Already processed: {len(processed_actor_names)}\")\n",
    "    print(f\"Remaining to process: {len(remaining_actors)}\")\n",
    "    \n",
    "    for actor in remaining_actors:\n",
    "        print(f\"Processing actor: {actor}\")\n",
    "        \n",
    "        # Get actor's common role traits\n",
    "        role_traits = get_actor_role_traits(actor)\n",
    "        \n",
    "        # Check if API limit was reached\n",
    "        if role_traits == \"API_LIMIT_REACHED\":\n",
    "            print(\"Stopping processing due to API limit.\")\n",
    "            break\n",
    "        \n",
    "        # Process all images and extract facial features\n",
    "        facial_features = process_actor_images(actor, actor_paths[actor])\n",
    "        \n",
    "        # Create comprehensive profile\n",
    "        processed_actors[actor] = {\n",
    "            'role_traits': role_traits,\n",
    "            'aggregated_features': aggregate_features(facial_features)\n",
    "        }\n",
    "        \n",
    "        # Save progress after each actor\n",
    "        save_progress(processed_actors)\n",
    "\n",
    "        print(f\"Completed processing {actor}\")\n",
    "        \n",
    "        # Add a small delay to avoid hitting rate limits too quickly\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return processed_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:28:15.926374Z",
     "iopub.status.busy": "2025-05-16T09:28:15.926076Z",
     "iopub.status.idle": "2025-05-16T09:28:15.930450Z",
     "shell.execute_reply": "2025-05-16T09:28:15.929544Z",
     "shell.execute_reply.started": "2025-05-16T09:28:15.926351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    base_dir = \"/kaggle/input/indian-actor-images-dataset/Bollywood Actor Images/Bollywood Actor Images\"\n",
    "\n",
    "    # Process actors until limit reached\n",
    "    processed_actors = process_actors_batch(\n",
    "        base_dir, \n",
    "    )\n",
    "\n",
    "    # Output current database status\n",
    "    print(f\"\\nCurrent database contains {len(processed_actors)} actors\")\n",
    "    print(f\"Remaining actors will be processed in the next run\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T09:28:17.861043Z",
     "iopub.status.busy": "2025-05-16T09:28:17.860734Z",
     "iopub.status.idle": "2025-05-16T09:28:17.874742Z",
     "shell.execute_reply": "2025-05-16T09:28:17.873895Z",
     "shell.execute_reply.started": "2025-05-16T09:28:17.861019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:31:16.419103Z",
     "iopub.status.busy": "2025-05-15T07:31:16.418757Z",
     "iopub.status.idle": "2025-05-15T07:31:16.464583Z",
     "shell.execute_reply": "2025-05-15T07:31:16.463784Z",
     "shell.execute_reply.started": "2025-05-15T07:31:16.419076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Role traits are not in clean format and are in String format, therefore converting them to JSON format\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load the raw JSON file\n",
    "with open(\"/kaggle/input/actorsfinal/processed_actors.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Iterate through each key (actor)\n",
    "for name, details in data.items():\n",
    "    raw_traits = details.get(\"role_traits\", None)\n",
    "\n",
    "    # Proceed only if role_traits is a string and contains list structure\n",
    "    if isinstance(raw_traits, str):\n",
    "        # Use regex to extract the JSON list part (between first '[' and last ']')\n",
    "        match = re.search(r'\\[\\s*{.*?}\\s*\\]', raw_traits, re.DOTALL)\n",
    "        if match:\n",
    "            json_like_str = match.group(0)\n",
    "            try:\n",
    "                parsed_traits = json.loads(json_like_str)\n",
    "                data[name][\"role_traits\"] = parsed_traits\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[JSONDecodeError] Could not parse 'role_traits' for {name}: {e}\")\n",
    "        else:\n",
    "            print(f\"[Warning] No JSON array found in 'role_traits' for {name}\")\n",
    "    else:\n",
    "        print(f\"[Info] Skipped {name} (already parsed or not a string)\")\n",
    "\n",
    "# Save the cleaned data\n",
    "with open(\"cleaned_data.json\", \"w\") as out_file:\n",
    "    json.dump(data, out_file, indent=4)\n",
    "\n",
    "print(\"Cleaning complete. Output written to 'cleaned_data.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T09:46:46.622156Z",
     "iopub.status.busy": "2025-05-15T09:46:46.621744Z",
     "iopub.status.idle": "2025-05-15T09:46:50.981139Z",
     "shell.execute_reply": "2025-05-15T09:46:50.979975Z",
     "shell.execute_reply.started": "2025-05-15T09:46:46.622122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we are generating a summary parameter in json file having a summary of all the traits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T09:49:22.233945Z",
     "iopub.status.busy": "2025-05-15T09:49:22.233242Z",
     "iopub.status.idle": "2025-05-15T10:37:03.485946Z",
     "shell.execute_reply": "2025-05-15T10:37:03.485061Z",
     "shell.execute_reply.started": "2025-05-15T09:49:22.233917Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarizer model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Load the input data\n",
    "with open(\"/kaggle/input/actorsfinal/cleaned_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Loop through each actor\n",
    "for name, details in data.items():\n",
    "    summary_input_parts = []\n",
    "\n",
    "    # Role Traits\n",
    "    role_traits = details.get(\"role_traits\", [])\n",
    "    if isinstance(role_traits, list):\n",
    "        trait_text = \" \".join([f\"{trait['trait']}: {trait['description']}\" for trait in role_traits])\n",
    "        summary_input_parts.append(trait_text)\n",
    "\n",
    "    # Aggregated Features\n",
    "    agg = details.get(\"aggregated_features\", {})\n",
    "    if agg:\n",
    "        age = agg.get(\"age\", None)\n",
    "        dom_race = agg.get(\"dominant_race\", None)\n",
    "        dom_emotion = agg.get(\"dominant_emotion\", None)\n",
    "\n",
    "        # Basic summary of features\n",
    "        feature_text = f\"Age: {age}. Dominant Race: {dom_race}. Dominant Emotion: {dom_emotion}.\"\n",
    "        summary_input_parts.append(feature_text)\n",
    "\n",
    "        # Optional: Add full emotion scores\n",
    "        scores = agg.get(\"emotion_scores\", {})\n",
    "        if scores:\n",
    "            score_text = \"Emotion Scores - \" + \", \".join([f\"{k}: {round(v, 1)}%\" for k, v in scores.items()])\n",
    "            summary_input_parts.append(score_text)\n",
    "\n",
    "    # Combine all into one summarizable block\n",
    "    final_text = \" \".join(summary_input_parts)\n",
    "    final_text = final_text[:3000]  # limit input size if needed\n",
    "    print(f\"For {name}:  {final_text}\")\n",
    "\n",
    "    try:\n",
    "        summary = summarizer(final_text, max_length=300, min_length=150, do_sample=False)[0][\"summary_text\"]\n",
    "        data[name][\"summary\"] = summary\n",
    "        print(f\"[✓] Summary added for {name}\")\n",
    "        print(summary)\n",
    "    except Exception as e:\n",
    "        print(f\"[✗] Failed to summarize for {name}: {e}\")\n",
    "\n",
    "# Save to file\n",
    "with open(\"summarized_full_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "print(\" Summarization complete. Output saved to 'summarized_full_data.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T11:13:05.626760Z",
     "iopub.status.busy": "2025-05-15T11:13:05.626408Z",
     "iopub.status.idle": "2025-05-15T11:13:09.294040Z",
     "shell.execute_reply": "2025-05-15T11:13:09.293063Z",
     "shell.execute_reply.started": "2025-05-15T11:13:05.626737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.7.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 2.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/10.5 MB 3.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.4/10.5 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.1/10.5 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.7/10.5 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.2/10.5 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.0/10.5 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.5/10.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.8/10.5 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.9/10.5 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.7/10.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.0/10.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\n",
      "   -------- ------------------------------- 1/5 [huggingface-hub]\n",
      "   -------- ------------------------------- 1/5 [huggingface-hub]\n",
      "   -------- ------------------------------- 1/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [tokenizers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   ------------------------ --------------- 3/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   -------------------------------- ------- 4/5 [sentence-transformers]\n",
      "   ---------------------------------------- 5/5 [sentence-transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.33.0 safetensors-0.5.3 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.52.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.73.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shashank\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shashank\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T11:18:00.425143Z",
     "iopub.status.busy": "2025-05-15T11:18:00.424441Z",
     "iopub.status.idle": "2025-05-15T11:18:05.394103Z",
     "shell.execute_reply": "2025-05-15T11:18:05.393462Z",
     "shell.execute_reply.started": "2025-05-15T11:18:00.425112Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Actors:\n",
      "amol_palekar: similarity = 0.4214\n",
      "salman_khan: similarity = 0.4007\n",
      "dilip_kumar: similarity = 0.3877\n",
      "akshaye_khanna: similarity = 0.3843\n",
      "akshay_kumar: similarity = 0.3822\n"
     ]
    }
   ],
   "source": [
    "# In this we are embedding the summary and user query and finding the best cosine similarity.\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load summaries\n",
    "with open(\"summarized_full_data.json\", \"r\") as f:\n",
    "    actor_data = json.load(f)\n",
    "\n",
    "# Load sentence embedding model\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Extract summaries and corresponding actor names\n",
    "actor_summaries = {actor: details['summary'] for actor, details in actor_data.items()}\n",
    "\n",
    "# Function to recommend actors\n",
    "def recommend_actors(user_description, top_k=5):\n",
    "    # Get embedding for user query\n",
    "    query_embedding = model.encode(user_description, convert_to_tensor=True)\n",
    "\n",
    "    # Compute similarity between user query and each actor's summary\n",
    "    similarities = []\n",
    "    for actor, summary in actor_summaries.items():\n",
    "        summary_embedding = model.encode(summary, convert_to_tensor=True)\n",
    "        score = util.cos_sim(query_embedding, summary_embedding).item()\n",
    "        similarities.append((actor, score))\n",
    "\n",
    "    # Sort by similarity score and return top matches\n",
    "    top_matches = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return top_matches\n",
    "\n",
    "query = \"Army look, strong and bold personality and voice, Have aggressive eyes\"\n",
    "\n",
    "top_actors = recommend_actors(query)\n",
    "print(\"Recommended Actors:\")\n",
    "for actor, score in top_actors:\n",
    "    print(f\"{actor}: similarity = {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T11:25:28.197171Z",
     "iopub.status.busy": "2025-05-15T11:25:28.196785Z",
     "iopub.status.idle": "2025-05-15T11:25:28.205607Z",
     "shell.execute_reply": "2025-05-15T11:25:28.204376Z",
     "shell.execute_reply.started": "2025-05-15T11:25:28.197134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2nd WAY OF RECOMMENDATION i.e. just matching from JSON format instead of summaries\n",
    "\n",
    "def recommend_actors(actor_profiles, role_requirements, top_n=3):\n",
    "\n",
    "    scores = {}\n",
    "    \n",
    "    # Parse role requirements\n",
    "    req_traits = role_requirements.get('traits', [])\n",
    "    req_facial = role_requirements.get('facial_features', {})\n",
    "    \n",
    "    for actor_name, profile in actor_profiles.items():\n",
    "        score = 0\n",
    "        \n",
    "        # Score based on role traits\n",
    "        actor_traits = profile['role_traits']\n",
    "        for trait in req_traits:\n",
    "            if any(trait.lower() in actor_trait['trait'].lower() for actor_trait in actor_traits):\n",
    "                score += 1\n",
    "        \n",
    "        # Score based on facial features\n",
    "        agg_features = profile['aggregated_features']\n",
    "        \n",
    "        # Age proximity (if specified)\n",
    "        if 'age' in req_facial and 'age' in agg_features:\n",
    "            age_diff = abs(req_facial['age'] - agg_features['age'])\n",
    "            # Convert age difference to a score (closer is better)\n",
    "            age_score = max(0, 1 - (age_diff / 50))  # Normalize by 50 years\n",
    "            score += age_score * 2  # Weight age more heavily\n",
    "        \n",
    "        # Gender match (if specified)\n",
    "        if 'gender' in req_facial and 'gender' in agg_features:\n",
    "            if req_facial['gender'].lower() == agg_features['gender'].lower():\n",
    "                score += 2\n",
    "        \n",
    "        # Emotion match (if specified)\n",
    "        if 'dominant_emotion' in req_facial and 'dominant_emotion' in agg_features:\n",
    "            if req_facial['dominant_emotion'].lower() == agg_features['dominant_emotion'].lower():\n",
    "                score += 1\n",
    "        \n",
    "        # Other facial features\n",
    "        for feature in ['has_beard', 'eye_intensity', 'face_seriousness', 'strong_look']:\n",
    "            if feature in req_facial and feature in agg_features:\n",
    "                if isinstance(agg_features[feature], bool):\n",
    "                    # Boolean feature\n",
    "                    if req_facial[feature] == agg_features[feature]:\n",
    "                        score += 1\n",
    "                else:\n",
    "                    # Numeric feature - calculate proximity (closer is better)\n",
    "                    diff = abs(req_facial[feature] - agg_features[feature])\n",
    "                    feature_score = max(0, 1 - diff)  # Normalize to 0-1\n",
    "                    score += feature_score\n",
    "        \n",
    "        scores[actor_name] = score\n",
    "    \n",
    "    # Sort actors by score and return top N\n",
    "    top_actors = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    return top_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T11:28:26.356742Z",
     "iopub.status.busy": "2025-05-15T11:28:26.356405Z",
     "iopub.status.idle": "2025-05-15T11:28:26.371657Z",
     "shell.execute_reply": "2025-05-15T11:28:26.370626Z",
     "shell.execute_reply.started": "2025-05-15T11:28:26.356713Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding best actors for the role...\n",
      "\n",
      "Top Recommended Actors:\n",
      "1. atul_kulkarni (Score: 3.98)\n",
      "2. kay_kay_menon (Score: 3.96)\n",
      "3. vinod_khanna (Score: 3.95)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "actor_profiles = {}\n",
    "    \n",
    "# Load actor profiles if exists\n",
    "if os.path.exists(\"cleaned_data.json\"):\n",
    "    with open(\"cleaned_data.json\", \"r\") as f:\n",
    "        actor_profiles = json.load(f)\n",
    "\n",
    "# Example role requirement for testing recommendations\n",
    "if len(actor_profiles) > 0:\n",
    "    example_role = {\n",
    "        'traits': ['intense', 'authoritative', 'Strong and bold voice', 'Rajputana Look'],\n",
    "        'facial_features': {\n",
    "            'age': 40,\n",
    "            'gender': 'Male',\n",
    "            'has_beard': False,\n",
    "            'eye_intensity': 0.8,\n",
    "            'face_seriousness': 0.7,\n",
    "            'strong_look': 0.9,\n",
    "            'dominant_emotion': 'angry'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Get recommendations\n",
    "    print(\"\\nFinding best actors for the role...\")\n",
    "    recommendations = recommend_actors(actor_profiles, example_role, top_n=3)\n",
    "    \n",
    "    # Display recommendations\n",
    "    print(\"\\nTop Recommended Actors:\")\n",
    "    for i, (actor, score) in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {actor} (Score: {score:.2f})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Since The above 2 recommendations are giving different result , therefore approach is to get Hybrid of these 2 recommendation functions and combined score, then result of Top n actors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T11:54:42.808217Z",
     "iopub.status.busy": "2025-05-15T11:54:42.807781Z",
     "iopub.status.idle": "2025-05-15T11:54:51.151510Z",
     "shell.execute_reply": "2025-05-15T11:54:51.150832Z",
     "shell.execute_reply.started": "2025-05-15T11:54:42.808181Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top Hybrid Recommended Actors:\n",
      "1. kay_kay_menon — Score: 0.4581\n",
      "2. prakash_raj — Score: 0.4555\n",
      "3. ajay_devgn — Score: 0.4387\n",
      "4. raj_babbar — Score: 0.4377\n",
      "5. atul_kulkarni — Score: 0.4271\n"
     ]
    }
   ],
   "source": [
    "# Load structured profiles\n",
    "with open(\"cleaned_data.json\", \"r\") as f:\n",
    "    actor_profiles = json.load(f)\n",
    "\n",
    "# Load summarized text data\n",
    "with open(\"summarized_full_data.json\", \"r\") as f:\n",
    "    actor_data = json.load(f)\n",
    "actor_summaries = {actor: details['summary'] for actor, details in actor_data.items()}\n",
    "\n",
    "# Load sentence transformer model\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Structured Matching Function\n",
    "# ------------------------------\n",
    "def compute_trait_score(actor_traits, query_traits):\n",
    "    match_count = sum(1 for qt in query_traits if any(qt.lower() in at['trait'].lower() for at in actor_traits))\n",
    "    return match_count / len(query_traits) if query_traits else 0\n",
    "\n",
    "\n",
    "def compute_feature_score(actor_features, query_features):\n",
    "    score = 0\n",
    "    max_score = 0\n",
    "\n",
    "    # Age similarity\n",
    "    if 'age' in actor_features and 'age' in query_features:\n",
    "        age_diff = abs(actor_features['age'] - query_features['age'])\n",
    "        age_score = max(0, 1 - age_diff / 30)  # normalize\n",
    "        score += age_score\n",
    "        max_score += 1\n",
    "\n",
    "    # Dominant emotion match\n",
    "    if 'dominant_emotion' in actor_features and 'dominant_emotion' in query_features:\n",
    "        score += int(actor_features['dominant_emotion'] == query_features['dominant_emotion'])\n",
    "        max_score += 1\n",
    "\n",
    "    return score / max_score if max_score else 0\n",
    "\n",
    "\n",
    "def get_structured_score(actor_name, role):\n",
    "    traits_score = compute_trait_score(actor_profiles[actor_name]['role_traits'], role['traits'])\n",
    "    features_score = compute_feature_score(actor_profiles[actor_name]['aggregated_features'], role['facial_features'])\n",
    "    return (traits_score + features_score) / 2\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Embedding Similarity Function\n",
    "# ------------------------------\n",
    "def get_summary_similarity(actor_name, role_text):\n",
    "    actor_summary = actor_summaries.get(actor_name, \"\")\n",
    "    if not actor_summary:\n",
    "        return 0\n",
    "    query_embedding = model.encode(role_text, convert_to_tensor=True)\n",
    "    summary_embedding = model.encode(actor_summary, convert_to_tensor=True)\n",
    "    return util.cos_sim(query_embedding, summary_embedding).item()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Final Recommender\n",
    "# ------------------------------\n",
    "def hybrid_recommend_actors(role, role_text_description, top_n=5, w_structured=0.6, w_text=0.4):\n",
    "    scores = []\n",
    "\n",
    "    for actor_name in actor_profiles.keys():\n",
    "        structured_score = get_structured_score(actor_name, role)\n",
    "        text_score = get_summary_similarity(actor_name, role_text_description)\n",
    "        final_score = w_structured * structured_score + w_text * text_score #Here taking 60% weightage to structured data ad 40% weightage to Summary data\n",
    "        scores.append((actor_name, final_score))\n",
    "\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores[:top_n]\n",
    "\n",
    "\n",
    "example_role = {\n",
    "    'traits': ['intense', 'authoritative', 'Strong and bold voice', 'Rajputana Look'],\n",
    "    'facial_features': {\n",
    "        'age': 40,\n",
    "        'gender': 'Male',\n",
    "        'has_beard': False,\n",
    "        'eye_intensity': 0.8,\n",
    "        'face_seriousness': 0.7,\n",
    "        'strong_look': 0.9,\n",
    "        'dominant_emotion': 'angry'\n",
    "    }\n",
    "}\n",
    "role_text = \"Army look, strong and bold personality and voice, aggressive eyes, Rajputana heritage\"\n",
    "\n",
    "print(\"\\n Top Hybrid Recommended Actors:\")\n",
    "top_actors = hybrid_recommend_actors(example_role, role_text)\n",
    "for i, (actor, score) in enumerate(top_actors, 1):\n",
    "    print(f\"{i}. {actor} — Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1280375,
     "sourceId": 3952819,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7423719,
     "sourceId": 11819515,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
